{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79d99ef",
   "metadata": {
    "id": "f79d99ef"
   },
   "source": [
    "# Train your first 🐸 TTS model 💫\n",
    "\n",
    "### 👋 Hello and welcome to Coqui (🐸) TTS\n",
    "\n",
    "The goal of this notebook is to show you a **typical workflow** for **training** and **testing** a TTS model with 🐸.\n",
    "\n",
    "Let's train a very small model on a very small amount of data so we can iterate quickly.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Download data and format it for 🐸 TTS.\n",
    "2. Configure the training and testing runs.\n",
    "3. Train a new model.\n",
    "4. Test the model and display its performance.\n",
    "\n",
    "So, let's jump right in!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2aec78",
   "metadata": {
    "id": "fa2aec78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\amori\\anaconda3\\lib\\site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-23.2.1-py3-none-any.whl (2.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\amori\\anaconda3\\python.exe -m pip install -U pip\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TTS\n",
      "  Downloading TTS-0.16.6.tar.gz (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 8.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting umap-learn==0.5.1\n",
      "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
      "     ---------------------------------------- 80.9/80.9 kB 4.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.2/42.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (4.64.1)\n",
      "Collecting bangla==0.0.2\n",
      "  Downloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
      "Collecting encodec\n",
      "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 8.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.0.2-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 8.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec>=2021.04.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (2022.11.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (3.7)\n",
      "Requirement already satisfied: flask in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (2.2.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (1.5.3)\n",
      "Collecting bnunicodenormalizer==0.1.1\n",
      "  Downloading bnunicodenormalizer-0.1.1.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "     -------------------------------------- 289.9/289.9 kB 8.7 MB/s eta 0:00:00\n",
      "Collecting trainer\n",
      "  Downloading trainer-0.0.31-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.6/50.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (3.7.2)\n",
      "Collecting numba==0.57.0\n",
      "  Downloading numba-0.57.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-win_amd64.whl (323 kB)\n",
      "     -------------------------------------- 323.1/323.1 kB 9.8 MB/s eta 0:00:00\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "     ---------------------------------------- 19.2/19.2 MB 9.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (4.24.0)\n",
      "Collecting jamo\n",
      "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: librosa==0.10.0.* in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (0.10.0.post2)\n",
      "Requirement already satisfied: soundfile in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (0.12.1)\n",
      "Collecting pypinyin\n",
      "  Downloading pypinyin-0.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 12.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (22.0)\n",
      "Collecting g2pkk>=0.1.1\n",
      "  Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
      "Collecting gruut[de,es,fr]==2.2.3\n",
      "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
      "     ---------------------------------------- 73.5/73.5 kB 4.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting numpy==1.22.0\n",
      "  Using cached numpy-1.22.0-cp310-cp310-win_amd64.whl (14.7 MB)\n",
      "Collecting scipy>=1.11.2\n",
      "  Downloading scipy-1.11.2-cp310-cp310-win_amd64.whl (44.0 MB)\n",
      "     ---------------------------------------- 44.0/44.0 MB 5.7 MB/s eta 0:00:00\n",
      "Collecting k-diffusion\n",
      "  Downloading k_diffusion-0.0.16-py3-none-any.whl (25 kB)\n",
      "Collecting cython==0.29.30\n",
      "  Using cached Cython-0.29.30-py2.py3-none-any.whl (985 kB)\n",
      "Requirement already satisfied: torch>=1.7 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (1.12.1)\n",
      "Collecting bnnumerizer\n",
      "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pysbd\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.1/71.1 kB ? eta 0:00:00\n",
      "Collecting inflect==5.6.0\n",
      "  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting coqpit>=0.0.16\n",
      "  Downloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\amori\\anaconda3\\lib\\site-packages (from TTS) (6.0)\n",
      "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.11.0)\n",
      "Collecting dateparser~=1.1.0\n",
      "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
      "     -------------------------------------- 293.8/293.8 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting gruut-ipa<1.0,>=0.12.0\n",
      "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
      "     -------------------------------------- 101.6/101.6 kB 6.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gruut_lang_en~=2.0.0\n",
      "  Downloading gruut_lang_en-2.0.0.tar.gz (15.2 MB)\n",
      "     --------------------------------------- 15.2/15.2 MB 10.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonlines~=1.2.0\n",
      "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: networkx<3.0.0,>=2.5.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.8.4)\n",
      "Collecting num2words<1.0.0,>=0.5.10\n",
      "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
      "     -------------------------------------- 125.2/125.2 kB 7.7 MB/s eta 0:00:00\n",
      "Collecting python-crfsuite~=0.9.7\n",
      "  Downloading python_crfsuite-0.9.9-cp310-cp310-win_amd64.whl (139 kB)\n",
      "     -------------------------------------- 139.4/139.4 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting gruut_lang_de~=2.0.0\n",
      "  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n",
      "     ---------------------------------------- 18.1/18.1 MB 7.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gruut_lang_es~=2.0.0\n",
      "  Downloading gruut_lang_es-2.0.0.tar.gz (31.4 MB)\n",
      "     ---------------------------------------- 31.4/31.4 MB 8.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gruut_lang_fr~=2.0.0\n",
      "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
      "     ---------------------------------------- 10.9/10.9 MB 4.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from librosa==0.10.0.*->TTS) (1.0.3)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from librosa==0.10.0.*->TTS) (0.3.5)\n",
      "Collecting librosa==0.10.0.*\n",
      "  Downloading librosa-0.10.0.post1-py3-none-any.whl (252 kB)\n",
      "     -------------------------------------- 253.0/253.0 kB 7.6 MB/s eta 0:00:00\n",
      "  Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
      "     -------------------------------------- 252.9/252.9 kB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from librosa==0.10.0.*->TTS) (4.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from librosa==0.10.0.*->TTS) (1.2.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from librosa==0.10.0.*->TTS) (3.0.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from librosa==0.10.0.*->TTS) (5.1.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from librosa==0.10.0.*->TTS) (0.3)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from librosa==0.10.0.*->TTS) (1.4.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from librosa==0.10.0.*->TTS) (1.1.1)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0\n",
      "  Downloading llvmlite-0.40.1-cp310-cp310-win_amd64.whl (27.7 MB)\n",
      "     ---------------------------------------- 27.7/27.7 MB 7.6 MB/s eta 0:00:00\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 9.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from soundfile->TTS) (1.15.1)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from aiohttp->TTS) (2.0.4)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp310-cp310-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.0/61.0 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from aiohttp->TTS) (22.1.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 44.4/44.4 kB ? eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from flask->TTS) (8.0.4)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from flask->TTS) (2.2.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from flask->TTS) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from flask->TTS) (2.0.1)\n",
      "Collecting jsonmerge\n",
      "  Downloading jsonmerge-1.9.2-py3-none-any.whl (19 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "     -------------------------------------- 251.2/251.2 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting resize-right\n",
      "  Downloading resize_right-0.0.2-py3-none-any.whl (8.9 kB)\n",
      "Collecting torchdiffeq\n",
      "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 5.1 MB/s eta 0:00:00\n",
      "Collecting clean-fid\n",
      "  Downloading clean_fid-0.1.35-py3-none-any.whl (26 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.15.10-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-image in c:\\users\\amori\\anaconda3\\lib\\site-packages (from k-diffusion->TTS) (0.19.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\amori\\anaconda3\\lib\\site-packages (from k-diffusion->TTS) (9.4.0)\n",
      "Collecting kornia\n",
      "  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n",
      "     -------------------------------------- 705.7/705.7 kB 8.9 MB/s eta 0:00:00\n",
      "Collecting clip-anytorch\n",
      "  Downloading clip_anytorch-2.5.2-py3-none-any.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 8.7 MB/s eta 0:00:00\n",
      "Collecting torchsde\n",
      "  Downloading torchsde-0.2.5-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 59.2/59.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from matplotlib->TTS) (4.25.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from matplotlib->TTS) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from matplotlib->TTS) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from matplotlib->TTS) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from matplotlib->TTS) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from matplotlib->TTS) (1.4.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from nltk->TTS) (2022.7.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from pandas->TTS) (2022.7)\n",
      "Collecting torch>=1.7\n",
      "  Downloading torch-2.0.1-cp310-cp310-win_amd64.whl (172.3 MB)\n",
      "     -------------------------------------- 172.3/172.3 MB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\amori\\anaconda3\\lib\\site-packages (from torch>=1.7->TTS) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\amori\\anaconda3\\lib\\site-packages (from torch>=1.7->TTS) (1.11.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tqdm->TTS) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\amori\\anaconda3\\lib\\site-packages (from trainer->TTS) (5.9.0)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\amori\\anaconda3\\lib\\site-packages (from trainer->TTS) (2.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from transformers->TTS) (0.10.1)\n",
      "Requirement already satisfied: requests in c:\\users\\amori\\anaconda3\\lib\\site-packages (from transformers->TTS) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from transformers->TTS) (0.11.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\amori\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile->TTS) (2.21)\n",
      "Collecting tzlocal\n",
      "  Downloading tzlocal-5.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->flask->TTS) (2.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\amori\\anaconda3\\lib\\site-packages (from jsonlines~=1.2.0->gruut[de,es,fr]==2.2.3->TTS) (1.16.0)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: appdirs in c:\\users\\amori\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa==0.10.0.*->TTS) (1.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa==0.10.0.*->TTS) (2.2.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->TTS) (3.4)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.1/53.1 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonschema>2.4.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from jsonmerge->k-diffusion->TTS) (4.17.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from requests->transformers->TTS) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from requests->transformers->TTS) (2022.12.7)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from scikit-image->k-diffusion->TTS) (2.26.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from scikit-image->k-diffusion->TTS) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from scikit-image->k-diffusion->TTS) (1.4.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from sympy->torch>=1.7->TTS) (1.2.1)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tensorboard->trainer->TTS) (4.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tensorboard->trainer->TTS) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tensorboard->trainer->TTS) (2.20.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tensorboard->trainer->TTS) (65.6.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tensorboard->trainer->TTS) (0.38.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tensorboard->trainer->TTS) (3.4.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tensorboard->trainer->TTS) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tensorboard->trainer->TTS) (1.54.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from tensorboard->trainer->TTS) (0.7.1)\n",
      "Collecting trampoline>=0.1.2\n",
      "  Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
      "Collecting boltons>=20.2.1\n",
      "  Downloading boltons-23.0.0-py2.py3-none-any.whl (194 kB)\n",
      "     ------------------------------------- 194.8/194.8 kB 11.5 MB/s eta 0:00:00\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.30.0-py2.py3-none-any.whl (218 kB)\n",
      "     ------------------------------------- 218.8/218.8 kB 13.9 MB/s eta 0:00:00\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-win_amd64.whl (11 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.35-py3-none-any.whl (188 kB)\n",
      "     ------------------------------------- 188.8/188.8 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.7/62.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->trainer->TTS) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->trainer->TTS) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->trainer->TTS) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->trainer->TTS) (1.3.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->TTS) (0.18.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from ftfy->clip-anytorch->k-diffusion->TTS) (0.2.5)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     -------------------------------------- 341.8/341.8 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->trainer->TTS) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\amori\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->trainer->TTS) (3.2.2)\n",
      "Building wheels for collected packages: TTS, bnunicodenormalizer, umap-learn, bnnumerizer, encodec, jieba, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr, pynndescent, gruut, docopt, pathtools\n",
      "  Building wheel for TTS (pyproject.toml): started\n",
      "  Building wheel for TTS (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for TTS: filename=TTS-0.16.6-cp310-cp310-win_amd64.whl size=811160 sha256=75bd456592e073024f09fb10d361b5339489ec677d8ab2bfbcbbc748cbf2888f\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\57\\42\\f5\\72adb062570b7118cae9abdec2d0cbcd422c76c1e5c09ad48c\n",
      "  Building wheel for bnunicodenormalizer (setup.py): started\n",
      "  Building wheel for bnunicodenormalizer (setup.py): finished with status 'done'\n",
      "  Created wheel for bnunicodenormalizer: filename=bnunicodenormalizer-0.1.1-py3-none-any.whl size=21954 sha256=ae4bcd864b3bbbfdb49abbf5fe4c459f94e02ec4ce03bfd07a837526308098f7\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\6e\\79\\03\\74c6b346d91628795ffee366602ca40678970f148138fb6108\n",
      "  Building wheel for umap-learn (setup.py): started\n",
      "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76642 sha256=6cf6bd7db2679f5710c03c46915dd8aff1b72b4c0b2a57a10cf73ffe343a4742\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\c2\\cb\\be\\491b6cf7430c6e4dbcbcde67b0aaf6a52a42ca852464a6dec5\n",
      "  Building wheel for bnnumerizer (setup.py): started\n",
      "  Building wheel for bnnumerizer (setup.py): finished with status 'done'\n",
      "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5274 sha256=cb3afe163e3e84de9074ec3bbb714bad1eeb1e05bc4d42bd1c57d95cdf77369e\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\ad\\e7\\c8\\b1fc6ede9cec5bba32e2baeb9cf63ef4d6ab54eadf8a961bad\n",
      "  Building wheel for encodec (setup.py): started\n",
      "  Building wheel for encodec (setup.py): finished with status 'done'\n",
      "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45806 sha256=c982511294f2a0bbaf3d180b0823612986ed7d45b9ea99cfb5854c6ff8055952\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\ce\\7b\\9e\\791357a98076b1d06f7113db0cb70a217c3feb31962366ab9d\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314474 sha256=4a9cb5796039583a6e1297acb749561fc10fd029ce64b5743aaab924578c0d33\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\fe\\aa\\79\\217233b7c4512e033cb980e378a27a73ff3c5359d2a48740f5\n",
      "  Building wheel for gruut-ipa (setup.py): started\n",
      "  Building wheel for gruut-ipa (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104908 sha256=6699312dc0005b1f4d1f5973a18879c3bbbc6bfe272bf31000fca1a570d4676e\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\aa\\68\\42\\dd64af2350164ca0a0f737272c083d4e99c59a91990802698b\n",
      "  Building wheel for gruut_lang_de (setup.py): started\n",
      "  Building wheel for gruut_lang_de (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498188 sha256=8b947e7ba6ab036ba9d0f41ced44d879e2a22502af3ecb94007dabacd3d15a3f\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\cd\\0a\\cb\\ec6b3b3c1829a9102fbc1306b6777bf2a783236a30433865c4\n",
      "  Building wheel for gruut_lang_en (setup.py): started\n",
      "  Building wheel for gruut_lang_en (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.0-py3-none-any.whl size=15297187 sha256=f70d849ac0e6e6fa7ef57543f5a619371622c3cd30a23981986975bbc5c75018\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\30\\78\\3c\\ce7d95d361d4ac84e9dabec90d3e07f07116c6d902eb13c0f7\n",
      "  Building wheel for gruut_lang_es (setup.py): started\n",
      "  Building wheel for gruut_lang_es (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.0-py3-none-any.whl size=32173805 sha256=775dfb40d8f8dc74e49d28096ce8b2f549198268cea29da8503233ec871d2837\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\ef\\e7\\e5\\d2659f83923c310e2016413329e09a50eb39424f96ff16e3f1\n",
      "  Building wheel for gruut_lang_fr (setup.py): started\n",
      "  Building wheel for gruut_lang_fr (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968774 sha256=f3b31f8ec1b037c0542430cf9f0734ffe7e37e9167b03dab11bf0a1c834dc869\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\c3\\de\\1a\\77322ecd9860f57918a6804e91d86e04945a03a51a22e2759a\n",
      "  Building wheel for pynndescent (setup.py): started\n",
      "  Building wheel for pynndescent (setup.py): finished with status 'done'\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55676 sha256=1cd14ac2d3100b76139462a28bb5d4e98092005cc01ebac48e86b78e75efa638\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\45\\f6\\1b\\b2cbeeaddcd5e454a300424e9bc30d5dcd2baaa7b94cfa2795\n",
      "  Building wheel for gruut (setup.py): started\n",
      "  Building wheel for gruut (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75859 sha256=3fb90d5e1de303faa69249185ce112ef6acbbc0ebbf0fc9beeab764c82085e23\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\87\\0c\\0c\\02634236833860cdec4110e05c4ea7ad98afee2f2c8972fe2c\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13775 sha256=2de8aaef731cefe6c94f9f8312ee2ca9b2e850ab7d9b3af979e596053d3ac5e5\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\7c\\d7\\8d\\2156234738063e3d4a39ba77dc677046100e62766b53807189\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8801 sha256=46e01c5d67a814d70826845fa273865edc2aad4edfe6e76d48c65b26fdb80cc8\n",
      "  Stored in directory: c:\\users\\amori\\appdata\\local\\pip\\cache\\wheels\\44\\1b\\54\\249c94316d4e1030e2d0683fba1d8ea06197de866f5a4de738\n",
      "Successfully built TTS bnunicodenormalizer umap-learn bnnumerizer encodec jieba gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr pynndescent gruut docopt pathtools\n",
      "Installing collected packages: trampoline, resize-right, python-crfsuite, pathtools, jieba, jamo, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, boltons, bnunicodenormalizer, bnnumerizer, bangla, tzdata, smmap, setproctitle, sentry-sdk, pysbd, pypinyin, numpy, num2words, multidict, llvmlite, jsonlines, inflect, gruut-ipa, ftfy, frozenlist, einops, docker-pycreds, cython, coqpit, async-timeout, anyascii, yarl, tzlocal, torch, scipy, numba, jsonmerge, gitdb, aiosignal, torchvision, torchsde, torchdiffeq, torchaudio, kornia, GitPython, g2pkk, dateparser, aiohttp, accelerate, wandb, pynndescent, librosa, gruut, encodec, clip-anytorch, clean-fid, umap-learn, trainer, k-diffusion, TTS\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.39.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\amori\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "## Install Coqui TTS\n",
    "! pip install -U pip\n",
    "! pip install TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fe49c",
   "metadata": {
    "id": "be5fe49c"
   },
   "source": [
    "## ✅ Data Preparation\n",
    "\n",
    "### **First things first**: we need some data.\n",
    "\n",
    "We're training a Text-to-Speech model, so we need some _text_ and we need some _speech_. Specificially, we want _transcribed speech_. The speech must be divided into audio clips and each clip needs transcription. More details about data requirements such as recording characteristics, background noise and vocabulary coverage can be found in the [🐸TTS documentation](https://tts.readthedocs.io/en/latest/formatting_your_dataset.html).\n",
    "\n",
    "If you have a single audio file and you need to **split** it into clips. It is also important to use a lossless audio file format to prevent compression artifacts. We recommend using **wav** file format.\n",
    "\n",
    "The data format we will be adopting for this tutorial is taken from the widely-used  **LJSpeech** dataset, where **waves** are collected under a folder:\n",
    "\n",
    "<span style=\"color:purple;font-size:15px\">\n",
    "/wavs<br />\n",
    " &emsp;| - audio1.wav<br />\n",
    " &emsp;| - audio2.wav<br />\n",
    " &emsp;| - audio3.wav<br />\n",
    "  ...<br />\n",
    "</span>\n",
    "\n",
    "and a **metadata.csv** file will have the audio file name in parallel to the transcript, delimited by `|`:\n",
    "\n",
    "<span style=\"color:purple;font-size:15px\">\n",
    "# metadata.csv <br />\n",
    "audio1|This is my sentence. <br />\n",
    "audio2|This is maybe my sentence. <br />\n",
    "audio3|This is certainly my sentence. <br />\n",
    "audio4|Let this be your sentence. <br />\n",
    "...\n",
    "</span>\n",
    "\n",
    "In the end, we should have the following **folder structure**:\n",
    "\n",
    "<span style=\"color:purple;font-size:15px\">\n",
    "/MyTTSDataset <br />\n",
    "&emsp;| <br />\n",
    "&emsp;| -> metadata.csv<br />\n",
    "&emsp;| -> /wavs<br />\n",
    "&emsp;&emsp;| -> audio1.wav<br />\n",
    "&emsp;&emsp;| -> audio2.wav<br />\n",
    "&emsp;&emsp;| ...<br />\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69501a10-3b53-4e75-ae66-90221d6f2271",
   "metadata": {
    "id": "69501a10-3b53-4e75-ae66-90221d6f2271"
   },
   "source": [
    "🐸TTS already provides tooling for the _LJSpeech_. if you use the same format, you can start training your models right away. <br />\n",
    "\n",
    "After you collect and format your dataset, you need to check two things. Whether you need a **_formatter_** and a **_text_cleaner_**. <br /> The **_formatter_** loads the text file (created above) as a list and the **_text_cleaner_** performs a sequence of text normalization operations that converts the raw text into the spoken representation (e.g. converting numbers to text, acronyms, and symbols to the spoken format).\n",
    "\n",
    "If you use a different dataset format then the LJSpeech or the other public datasets that 🐸TTS supports, then you need to write your own **_formatter_** and  **_text_cleaner_**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f226c8-4e55-48fa-937b-8415d539b17c",
   "metadata": {
    "id": "e7f226c8-4e55-48fa-937b-8415d539b17c"
   },
   "source": [
    "## ⏳️ Loading your dataset\n",
    "Load one of the dataset supported by 🐸TTS.\n",
    "\n",
    "We will start by defining dataset config and setting LJSpeech as our target dataset and define its path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66",
   "metadata": {
    "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "\n",
    "output_path = \"tts_train_dir\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b7019-3685-4b48-8917-c152e288d7e3",
   "metadata": {
    "id": "ae6b7019-3685-4b48-8917-c152e288d7e3"
   },
   "outputs": [],
   "source": [
    "# Download and extract LJSpeech dataset.\n",
    "\n",
    "!wget -O $output_path/LJSpeech-1.1.tar.bz2 https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "!tar -xf $output_path/LJSpeech-1.1.tar.bz2 -C $output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd3ab5-6387-45f1-b488-24734cc1beb5",
   "metadata": {
    "id": "76cd3ab5-6387-45f1-b488-24734cc1beb5"
   },
   "outputs": [],
   "source": [
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"LJSpeech-1.1/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82fd75",
   "metadata": {
    "id": "ae82fd75"
   },
   "source": [
    "## ✅ Train a new model\n",
    "\n",
    "Let's kick off a training run 🚀🚀🚀.\n",
    "\n",
    "Deciding on the model architecture you'd want to use is based on your needs and available resources. Each model architecture has it's pros and cons that define the run-time efficiency and the voice quality.\n",
    "We have many recipes under `TTS/recipes/` that provide a good starting point. For this tutorial, we will be using `GlowTTS`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5876e46-2aee-4bcf-b6b3-9e3c535c553f",
   "metadata": {
    "id": "f5876e46-2aee-4bcf-b6b3-9e3c535c553f"
   },
   "source": [
    "We will begin by initializing the model training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84",
   "metadata": {
    "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84"
   },
   "outputs": [],
   "source": [
    "# GlowTTSConfig: all model related values for training, validating and testing.\n",
    "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
    "config = GlowTTSConfig(\n",
    "    batch_size=32,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    epochs=100,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=25,\n",
    "    print_eval=False,\n",
    "    mixed_precision=True,\n",
    "    output_path=output_path,\n",
    "    datasets=[dataset_config],\n",
    "    save_step=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ed377-80b7-447b-bd92-106bffa777ee",
   "metadata": {
    "id": "b93ed377-80b7-447b-bd92-106bffa777ee"
   },
   "source": [
    "Next we will initialize the audio processor which is used for feature extraction and audio I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b12f61-f851-4565-84dd-7640947e04ab",
   "metadata": {
    "id": "b1b12f61-f851-4565-84dd-7640947e04ab"
   },
   "outputs": [],
   "source": [
    "from TTS.utils.audio import AudioProcessor\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "# Modify sample rate if for a custom audio dataset:\n",
    "# ap.sample_rate = 22050\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d461683-b05e-403f-815f-8007bda08c38",
   "metadata": {
    "id": "1d461683-b05e-403f-815f-8007bda08c38"
   },
   "source": [
    "Next we will initialize the tokenizer which is used to convert text to sequences of token IDs.  If characters are not defined in the config, default characters are passed to the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014879b7-f18d-44c0-b24a-e10f8002113a",
   "metadata": {
    "id": "014879b7-f18d-44c0-b24a-e10f8002113a"
   },
   "outputs": [],
   "source": [
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3016e1-9e99-4c4f-94e3-fa89231fd978",
   "metadata": {
    "id": "df3016e1-9e99-4c4f-94e3-fa89231fd978"
   },
   "source": [
    "Next we will load data samples. Each sample is a list of ```[text, audio_file_path, speaker_name]```. You can define your custom sample loader returning the list of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7",
   "metadata": {
    "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7"
   },
   "outputs": [],
   "source": [
    "from TTS.tts.datasets import load_tts_samples\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19",
   "metadata": {
    "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19"
   },
   "source": [
    "Now we're ready to initialize the model.\n",
    "\n",
    "Models take a config object and a speaker manager as input. Config defines the details of the model like the number of layers, the size of the embedding, etc. Speaker manager is used by multi-speaker models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ffe3e-ad0c-443e-800c-9b076ee811b4",
   "metadata": {
    "id": "ac2ffe3e-ad0c-443e-800c-9b076ee811b4"
   },
   "outputs": [],
   "source": [
    "from TTS.tts.models.glow_tts import GlowTTS\n",
    "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2832c56-889d-49a6-95b6-eb231892ecc6",
   "metadata": {
    "id": "e2832c56-889d-49a6-95b6-eb231892ecc6"
   },
   "source": [
    "Trainer provides a generic API to train all the 🐸TTS models with all its perks like mixed-precision training, distributed training, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe",
   "metadata": {
    "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe"
   },
   "outputs": [],
   "source": [
    "from trainer import Trainer, TrainerArgs\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b320831-dd83-429b-bb6a-473f9d49d321",
   "metadata": {
    "id": "5b320831-dd83-429b-bb6a-473f9d49d321"
   },
   "source": [
    "### AND... 3,2,1... START TRAINING 🚀🚀🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f",
   "metadata": {
    "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f"
   },
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff0c40-2734-40a6-a905-e945a9fb3e98",
   "metadata": {
    "id": "4cff0c40-2734-40a6-a905-e945a9fb3e98"
   },
   "source": [
    "#### 🚀 Run the Tensorboard. 🚀\n",
    "On the notebook and Tensorboard, you can monitor the progress of your model. Also Tensorboard provides certain figures and sample outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85cd3b-1646-40ad-a6c2-49323e08eeec",
   "metadata": {
    "id": "5a85cd3b-1646-40ad-a6c2-49323e08eeec"
   },
   "outputs": [],
   "source": [
    "!pip install tensorboard\n",
    "!tensorboard --logdir=tts_train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6dc959",
   "metadata": {
    "id": "9f6dc959"
   },
   "source": [
    "## ✅ Test the model\n",
    "\n",
    "We made it! 🙌\n",
    "\n",
    "Let's kick off the testing run, which displays performance metrics.\n",
    "\n",
    "We're committing the cardinal sin of ML 😈 (aka - testing on our training data) so you don't want to deploy this model into production. In this notebook we're focusing on the workflow itself, so it's forgivable 😇\n",
    "\n",
    "You can see from the test output that our tiny model has overfit to the data, and basically memorized this one sentence.\n",
    "\n",
    "When you start training your own models, make sure your testing data doesn't include your training data 😅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fada7a-592f-4a09-9369-e6f3d82de3a0",
   "metadata": {
    "id": "99fada7a-592f-4a09-9369-e6f3d82de3a0"
   },
   "source": [
    "Let's get the latest saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd47ed5-da8e-4bf9-b524-d686630d6961",
   "metadata": {
    "id": "6dd47ed5-da8e-4bf9-b524-d686630d6961"
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "output_path = \"tts_train_dir\"\n",
    "ckpts = sorted([f for f in glob.glob(output_path+\"/*/*.pth\")])\n",
    "configs = sorted([f for f in glob.glob(output_path+\"/*/*.json\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42bc7a",
   "metadata": {
    "id": "dd42bc7a"
   },
   "outputs": [],
   "source": [
    " !tts --text \"Text for TTS\" \\\n",
    "      --model_path $test_ckpt \\\n",
    "      --config_path $test_config \\\n",
    "      --out_path out.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cbcb3f-d952-469b-a0d8-8941cd7af670",
   "metadata": {
    "id": "81cbcb3f-d952-469b-a0d8-8941cd7af670"
   },
   "source": [
    "## 📣 Listen to the synthesized wave 📣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000bd6-6763-4a10-a74d-911dd08ebcff",
   "metadata": {
    "id": "e0000bd6-6763-4a10-a74d-911dd08ebcff"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"out.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13914401-cad1-494a-b701-474e52829138",
   "metadata": {
    "id": "13914401-cad1-494a-b701-474e52829138"
   },
   "source": [
    "## 🎉 Congratulations! 🎉 You now have trained your first TTS model!\n",
    "Follow up with the next tutorials to learn more advanced material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d9fc6-896f-4a2c-86fd-8fd1fcbbb3f7",
   "metadata": {
    "id": "950d9fc6-896f-4a2c-86fd-8fd1fcbbb3f7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
